# PDF Word Frequency Analyzer

A high-performance, parallelized word frequency analyzer for processing single PDF files or entire directories of PDFs.
Designed for speed and scalability using multithreading (I/O) + multiprocessing (CPU).

---

## ğŸš€ Features

* Analyze a **single PDF** or an **entire folder**
* Fast **multithreaded PDF reading**
* Fast **multiprocessing word filtering**
* Optional **language filtering** (ISO 639-1, e.g. `en`, `tr`)
* Exclude words manually or via file
* Frequency filters:
    * Min/Max frequency range
    * Exact frequency values
* Export results in:
    * **TXT**
    * **CSV**
    * **JSON**
* Clean and modular codebase (`reader.py`, `processor.py`, `filters.py`, `cli.py`)

---

## ğŸ“¦ Installation

```bash
git clone https://github.com/mvu20002/pdf_word_frequency_analyzer.git
pip install -r requirements.txt
```

## ğŸ”§ Basic Usage

Analyze a directory containing PDF files:

```bash
python -m analyzer.cli ./pdfs
```

Analyze a single PDF:

```bash
python -m analyzer.cli ./documents/report.pdf
```

Default output is a .txt file named: `word_analysis_output.txt`

---

## ğŸ§¹ Excluding Words

Exclude words directly on the command line:

```bash
python -m analyzer.cli ./pdfs -e the and is of to
```

Exclude using a file (excluded.txt, one word per line):

```bash
python -m analyzer.cli ./pdfs -ef excluded.txt
```

Combine both:

```bash
python -m analyzer.cli ./pdfs -e the and -ef excluded.txt
```

---

## ğŸ“Š Frequency Filtering

Filter by frequency range (inclusive):

```bash
python -m analyzer.cli ./pdfs -r 5 10
```

This keeps only words appearing between 5 and 10 times.

Filter by exact frequencies:

```bash
python -m analyzer.cli ./pdfs -ex 3 7 15
```

This keeps only words that appear exactly 3, 7, or 15 times.

---

## ğŸŒ Language Filtering

Filter words by detected language (ISO 639-1):

```bash
python -m analyzer.cli ./pdfs -l en
```

Multiple languages:

```bash
python -m analyzer.cli ./pdfs -l en tr de
```

If not provided, no language filtering is applied.

---

## ğŸ“ Output Formats

**TXT (default):**

```bash
python -m analyzer.cli ./pdfs -o txt -fn results.txt
```

**CSV:**

```bash
python -m analyzer.cli ./pdfs -o csv -fn frequencies.csv
```

**JSON:**

```bash
python -m analyzer.cli ./pdfs -o json -fn output.json
```

---

## ğŸ§ª Example Commands

Analyze PDFs, exclude common words, keep only English, output JSON:

```bash
python -m analyzer.cli ./pdfs \
    -e the a an is are was were \
    -l en \
    -o json \
    -fn english_filtered.json
```

Full example with exact frequencies & exclusion file:

```bash
python -m analyzer.cli ./reports \
    -ef stopwords.txt \
    -ex 2 4 8 \
    -o csv \
    -fn freq_output.csv
```

---

## ğŸ“š Programmatic Usage

You can import and use the analyzer as a Python module:

```python
from analyzer.processor import calculate_word_frequency_and_filter, write_output

total_count, freq = calculate_word_frequency_and_filter(
    input_path="./docs",
    excluded_words=["the", "and"],
    target_lang_codes=["en"],
    min_freq=3,
    max_freq=20,
)

write_output(freq, total_count, output_format="json", filename="report.json")
```

---

## ğŸ—‚ Project Structure

```
your-repo/
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ __init__.py
â””â”€â”€ analyzer/
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ reader.py
    â”œâ”€â”€ processor.py
    â”œâ”€â”€ filters.py
    â””â”€â”€ cli.py
```

---

## ğŸ“„ License

MIT License.

---

## ğŸ¤ Contributions

Feel free to submit issues or pull requests!

Happy analyzing! ğŸ“˜ğŸ”
